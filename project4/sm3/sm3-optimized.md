# SM3软件优化分析

## 1\. 基础算法性能瓶颈分析

在进行任何优化之前，我们必须首先理解SM3算法的内部结构和性能瓶颈。SM3采用**Merkle-Damgård**结构，其核心是一个迭代的压缩函数。每一次哈希计算都包括以下三个主要阶段：

1.  **消息填充 (Padding)**：在消息末尾添加一个“1”，若干个“0”，以及64位的消息长度。这个操作的计算量很小，几乎可以忽略不计。
2.  **消息扩展 (Message Expansion)**：将一个64字节（512位）的消息块，扩展为68个32位字（$W\_0$到$W\_{67}$）和64个32位字（$W'*0$到$W'*{63}$）。这个过程涉及大量的位运算和异或操作。
3.  **压缩函数迭代 (Compression Iteration)**：一个包含64轮的循环，每轮都对8个32位的内部状态变量（A-H）进行更新。每一轮的计算都高度依赖于前一轮的结果，因此这是一个**高度序列化**的过程。

通过对这三个阶段的分析，我们可以确定优化的主要方向集中在**消息扩展**和**压缩函数**上。消息扩展的计算量大，且具有一定的并行性；压缩函数虽然序列化，但其内部的循环结构和位运算可以通过优化来加速。

## 2\. 优化过程

简单优化的目标是**减少不必要的计算和提高缓存效率**，主要通过**消息扩展预计算**和**循环展开**来实现。

### 2.1. 消息扩展预计算的推导

在原始的SM3实现中，消息扩展的计算公式为：
$W\_j = P\_1(W\_{j-16} \\oplus W\_{j-9} \\oplus \\text{ROL}(W\_{j-3}, 15)) \\oplus \\text{ROL}(W\_{j-13}, 7) \\oplus W\_{j-6}$
$W'*j = W\_j \\oplus W*{j+4}$

其中，$j \\in [16, 67]$，$W'\_j$的计算是在压缩函数的64轮循环中进行的。

**推导过程**：

假设我们的代码中有两个独立的循环：

1.  **消息扩展循环**：`for (j=16; j<68; j++) W[j] = ...;`
2.  **压缩函数循环**：`for (j=0; j<64; j++) { W1[j] = W[j] ^ W[j+4]; ... }`

这种实现方式会导致：

  * **内存访问不连续**：在压缩循环中，`W[j]`和`W[j+4]`可能不被缓存到同一行，导致缓存效率低下。
  * **冗余计算**：如果$W\_j'$的计算逻辑嵌入在压缩循环中，会导致每次压缩都需要重复计算$W\_j \\oplus W\_{j+4}$。

**优化方案**：
我们将$W\_j'$的计算与$W\_j$的计算合并。在消息扩展阶段，我们一次性计算出所有的$W\_j$和$W\_j'$，并将其存储在两个独立的数组中。

**推导后的伪代码**：

```c
// 原始的16个字从消息块B中加载
for (int j = 0; j < 16; ++j) {
    W[j] = load_from_block(B, j);
}

// 预先计算所有 W[16] 到 W[67]
for (int j = 16; j < 68; ++j) {
    W[j] = P1(W[j - 16] ^ W[j - 9] ^ ROL(W[j - 3], 15)) ^ ROL(W[j - 13], 7) ^ W[j - 6];
}

// 预先计算所有 W'[0] 到 W'[63]
for (int j = 0; j < 64; ++j) {
    W1[j] = W[j] ^ W[j + 4];
}

// 压缩函数循环，直接使用预计算好的W[j]和W1[j]
for (int j = 0; j < 64; ++j) {
    // ... 使用 W[j] 和 W1[j] ...
}
```

通过这种方式，我们显著减少了压缩循环中的内存访问和计算开销，提高了数据局部性。

### 2.2. 循环展开的分析

压缩函数有固定的64轮循环，其结构为 `for (j=0; j<64; j++) { ... }`。

**分析**：
每次循环迭代都会产生以下开销：

  * 循环变量`j`的自增操作。
  * 循环条件`j < 64`的判断。
  * 分支跳转指令（`jmp`）。

对于一个64次的循环，这些开销虽然每次都很小，但累积起来也会对性能产生影响。此外，现代CPU的**分支预测**机制也可能因为频繁的跳转而失效，导致流水线停顿。

**优化方案**：
我们将循环进行展开，将64次迭代分为多个较小的循环，甚至完全展开。例如，我们可以将64轮循环展开为16个一组，共4个循环：

**推导后的伪代码**：

```c
// 16轮循环展开
for (int j = 0; j < 16; ++j) {
    // 迭代 j
    // ...
}
for (int j = 16; j < 32; ++j) {
    // 迭代 j
    // ...
}
// ...以此类推，直到 j=63
```

这样做的好处是减少了分支预测的开销，使得CPU流水线能够更平滑地执行。但是，展开的程度过高会导致代码膨胀，增加指令缓存的压力，因此需要找到一个平衡点。

## 3\. 指令集并行优化（SIMD）

要实现SM3的SIMD优化，我们必须放弃对**单个**SM3任务的优化，转而专注于对**多个独立**SM3任务的并行处理。这是因为SM3压缩函数内部的序列依赖性太强，无法在单任务中有效利用SIMD。

### 3.1. 核心思想：多路并行哈希

我们的目标是同时处理4个独立的SM3哈希任务，利用AVX指令集（256位）一次性处理8个32位整数。

**推导过程**：

1.  **数据打包 (Data Packing)**：SM3有8个32位的状态变量A-H。为了并行处理4个任务，我们需要8个256位的AVX寄存器，每个寄存器存储4个任务的同一个状态变量。

      * 例如，`A_vec`寄存器将存储：
          * `A_vec.lane0`：任务1的A状态
          * `A_vec.lane1`：任务2的A状态
          * `A_vec.lane2`：任务3的A状态
          * `A_vec.lane3`：任务4的A状态
      * 同理，我们需要`B_vec, C_vec, ..., H_vec`等寄存器来存储其他状态。

2.  **指令选择**：SM3的核心计算包括加法、异或、左移和置换。AVX指令集提供了相应的并行指令：

      * **加法**：`_mm256_add_epi32`
      * **异或**：`_mm256_xor_si256`
      * **左移**：`_mm256_slli_epi32`
      * **位运算**：`_mm256_and_si256`, `_mm256_or_si256`, `_mm256_andnot_si256`
      * **数据重排**：`_mm256_permutevar8x32_epi32`等

3.  **消息扩展的并行化**：

      * 首先，将4个消息块的16个32位字加载到16个256位的AVX寄存器中。
      * 然后，利用AVX指令并行计算$W\_j$和$W'\_j$。例如，$P\_1$置换函数可以分解为异或和左移操作，这些操作都可以用AVX指令并行执行。

4.  **压缩函数的并行化**：

      * 使用`A_vec`到`H_vec`这8个寄存器存储4个任务的状态。
      * 在每一轮循环中，所有的位运算和加法都使用AVX指令并行执行。
      * 例如，计算SS1：
          * `ROL(A, 12)`可以分解为两次`_mm256_slli_epi32`和一次`_mm256_srli_epi32`。
          * `E + T_val`可以变为`_mm256_add_epi32(E_vec, T_vec)`。
          * $T\_{val}$在SM3中是固定的，因此可以预先加载到AVX寄存器中。

**伪代码**：

```c
sm3_compress_batch8(const uint8_t *block_ptrs[8], uint32_t out_states[8][8]) {
    __m256i A = _mm256_set1_epi32(0x7380166f);
    __m256i B = _mm256_set1_epi32(0x4914b2b9);
    __m256i C = _mm256_set1_epi32(0x172442d7);
    __m256i D = _mm256_set1_epi32(0xda8a0600);
    __m256i E = _mm256_set1_epi32(0xa96f30bc);
    __m256i F = _mm256_set1_epi32(0x163138aa);
    __m256i G = _mm256_set1_epi32(0xe38dee4d);
    __m256i H = _mm256_set1_epi32(0xb0fb0e4e);

    __m256i Wv[68];
    __m256i W1v[64];

    for (int j = 0; j < 16; j++) Wv[j] = load_word8_be(block_ptrs, j);
    for (int j = 16; j < 68; j++){
        __m256i tmp = _mm256_xor_si256(_mm256_xor_si256(Wv[j-16], Wv[j-9]), rotl32_avx2(Wv[j-3], 15));
        __m256i part = P1_avx2(tmp);
        Wv[j] = _mm256_xor_si256(_mm256_xor_si256(part, rotl32_avx2(Wv[j-13],7)), Wv[j-6]);
    }
    for (int j = 0; j < 64; j++) W1v[j] = _mm256_xor_si256(Wv[j], Wv[j+4]);

    for (int j = 0; j < 64; j++){
        uint32_t T = (j < 16) ? 0x79cc4519U : 0x7a879d8aU;
        uint32_t rot = ( (T << (j % 32)) | (T >> (32 - (j % 32))) );
        __m256i rotv = _mm256_set1_epi32((int)rot);

        __m256i SS1 = rotl32_avx2(_mm256_add_epi32(rotl32_avx2(A,12), _mm256_add_epi32(E, rotv)), 7);
        __m256i SS2 = _mm256_xor_si256(SS1, rotl32_avx2(A,12));

        __m256i TT1 = _mm256_add_epi32(_mm256_add_epi32(FF_avx2(A,B,C,j), D), _mm256_add_epi32(SS2, W1v[j]));
        __m256i TT2 = _mm256_add_epi32(_mm256_add_epi32(GG_avx2(E,F,G,j), H), _mm256_add_epi32(SS1, Wv[j]));

        D = C; C = rotl32_avx2(B, 9); B = A; A = TT1;
        H = G; G = rotl32_avx2(F, 19); F = E; E = P0_avx2(TT2);
    }

    // store lanes to out_states: lanes order in registers corresponds to set_epi32 arguments
    int32_t tmp[8];
    _mm256_storeu_si256((__m256i*)tmp, A);
    for (int lane = 0; lane < 8; lane++) out_states[lane][0] = (uint32_t)tmp[lane] ^ 0x7380166fU;
    _mm256_storeu_si256((__m256i*)tmp, B);
    for (int lane = 0; lane < 8; lane++) out_states[lane][1] = (uint32_t)tmp[lane] ^ 0x4914b2b9U;
    _mm256_storeu_si256((__m256i*)tmp, C);
    for (int lane = 0; lane < 8; lane++) out_states[lane][2] = (uint32_t)tmp[lane] ^ 0x172442d7U;
    _mm256_storeu_si256((__m256i*)tmp, D);
    for (int lane = 0; lane < 8; lane++) out_states[lane][3] = (uint32_t)tmp[lane] ^ 0xda8a0600U;
    _mm256_storeu_si256((__m256i*)tmp, E);
    for (int lane = 0; lane < 8; lane++) out_states[lane][4] = (uint32_t)tmp[lane] ^ 0xa96f30bcU;
    _mm256_storeu_si256((__m256i*)tmp, F);
    for (int lane = 0; lane < 8; lane++) out_states[lane][5] = (uint32_t)tmp[lane] ^ 0x163138aaU;
    _mm256_storeu_si256((__m256i*)tmp, G);
    for (int lane = 0; lane < 8; lane++) out_states[lane][6] = (uint32_t)tmp[lane] ^ 0xe38dee4dU;
    _mm256_storeu_si256((__m256i*)tmp, H);
    for (int lane = 0; lane < 8; lane++) out_states[lane][7] = (uint32_t)tmp[lane] ^ 0xb0fb0e4eU;
}
```

通过这种**多路并行**的方式，我们能够将吞吐量提升至理论上限（例如，使用AVX2可以提升4倍）。

## 4\. 优化效果与局限性讨论

### 4.1. 简单优化

  * **优点**：实现简单，对所有硬件平台都有效，无需特殊的指令集。可以显著减少CPU的控制流开销，提高缓存命中率。
  * **缺点**：性能提升有限，通常在20%-50%之间。无法突破算法本身的序列化瓶颈。

### 4.2. SIMD优化

  * **优点**：在批量处理任务中，性能提升巨大（理论上可达2-8倍，取决于指令集）。能够充分利用现代CPU的硬件并行能力。
  * **缺点**：
      * **适用场景受限**：仅适用于需要同时哈希多个独立消息的场景。
      * **单任务无提升**：如果只需要计算单个文件的哈希值，SIMD优化几乎没有效果。
      * **代码复杂性高**：需要使用晦涩难懂的编译器内置函数，代码可读性差，难以调试和维护。
      * **依赖特定指令集**：代码不具备跨平台通用性，需要针对不同的CPU架构（x86/AVX, ARM/NEON）编写不同的实现。

### 4.3. 总结

在实际应用中，我们会根据具体需求选择合适的优化方案。对于通用目的的库，简单优化是首选。而在高性能计算、大数据处理和并行文件系统等场景中，指令集优化则是实现吞吐量飞跃的关键。
